---
title: 'flower book: Optimization for Training DeepModels'
date: 2018-06-29 00:01:16
mathjax: true
tags:
	- machine learning
categories:
	- reading notes
---
换电脑把之前那个博客[apostlepaul.github.io](https://apostlepaul.github.io)的源码弄丢了，没有post了，懒得一篇篇恢复了，所以重新弄一个吧。
本章内容：
1. 学习和优化的区别
2. 神经网络优化中的挑战
3. 基本算法
4. 参数初始化策略
5. 自适应学习率算法
6. 二阶近似方法
7. 优化策略和元算法
<!--more-->

### 1. How Learning Diﬀers from Pure Optimization
深度模型训练的优化算法跟传统的优化算法不一样，主要体现在前者通常不是直接进行的。
具体来讲：我们关心的可能是测试集上某个难解的度量P，但是却需要通过降低一个代价函数J(θ)来间接地优化P。
典型的，我们会去优化训练集上的代价函数，即：
$$
J(\theta) = \mathbb{E}_{(x,y)~\hat{P}_{data}}L(f(x;\theta),y), \tag{1}\label{1}
$$
但其实，我们更期望优化的是数据生成分布pdata上的代价函数：
$$
J(\theta) = \mathbb{E}_{(x,y)~p_{data}}L(f(x;\theta),y). \tag{2}\label{2}
$$

#### 1.1  Empirical Risk Minimization
机器学习算法的目标是降低公式$\eqref{2}$所表达的泛化误差期望。这个量也叫做风险。
需要注意的是，假如我们知道真实的分布pdata(x,y)，那么最小化风险将会是一个优化算法问题。如果我们不知道这个真实的分布，而只有一个训练集，那么我们面对的是一个机器学习问题。
把机器学习问题转变为优化问题的最简单的办法就是最小化训练集上的期望损失。用训练集上的经验分布代替真实的分布：
$$
\mathbb{E}_{(x,y)~\hat{P}_{data}}L(f(x;\theta),y) = 1/m \Sigma^m_{i=1}L(f(x^{(i)};\sigma),y^{(i)}), \tag{3}\label{3}
$$
已经有一些理论证明，在满足一定的条件的时候，经验风险的降低会使风险降低。
但是，使用最小化经验风险的方法有两个问题：
1. 但是经验风险最小化容易导致过拟合，高容量的模型很容易就记住训练集了，
2. 一些有用的代价函数没有有用的微分，例如0-1loss，而大多有效的现代优化算法都是基于梯度下降的。

所以在深度学习的场景中，我们很少直接用经验风险最小化的方法，反之，我们会使用一个稍有不同的方法，我们真正优化的目标会更加不同于我们希望优化的目标。

#### 1.2 Surrogate Loss Functions and Early Stopping
我们经常会使用到代理损失函数，原因是：
1. 有时候真正的目标损失函数是不可解的，例如最简单的0-1分类，如果直接用0-1损失，那么这个优化算法将是输入维度的指数级；
2. 代理损失函数能学到更多，如训练集上的0-1损失已经为0的时候，代理损失函数的继续训练会提供测试集上的效果，会增强鲁棒性。

纯优化算法经常会在一个局部极小值点停止迭代，但是机器学习优化算法不会，机器学习优化算法满足了提前停止的条件（如在测试集上的真实损失函数已经ok）就会停止，而不care当前的代理损失函数的在停止时的梯度是大是小。

#### 1.3 Batch and Minibatch Algorithms
机器学习算法跟一般优化算法的一个区别是其目标函数同城可以分解为训练样本上的求和。
在实践中，通常用一部分样本来估计整个代价函数的期望，然后对参数进行更新。原因在于：
1. 增加样本数，计算代价增大，获得的收益却低于线性的。例如样本均值标准差为δ/(√n)，其中δ是样本真实标准差。分母表明了，样本数从100增加到100000，计算代价增加了100倍，收益却只有10倍。
2. 实际的训练集中有冗余，大量的样本对梯度的贡献是相似的。

有些算法对采样误差很敏感，可能有两个原因：
1. 算法使用的信息很难在小数据集上准确估计；
2. 使用了会放大采用误差的信息。

要保证minibatch是随机抽取的，因为现实中很多样本采集在顺序上是相关的。
从online learning我们可以看到SGD会减小泛化误差的原因：模型看到的每一个样本，都是从数据生成分布pdata(x,y)中得来的，没有重复。所以我们可以从mini batch中计算出泛化误差梯度的无偏估计。
分minibatch常用的做法是，讲数据打乱，分成相应的minibatch并存储，在模型训练的时候，用数据训练多遍模型。
需要注意的是只有在第一遍的时候，得到的是无偏估计，因为数据没有被重用，以后得到的都是有偏估计。但依然要训练多遍的原因是：多遍训练给训练误差带来的降低，与增加训练误差和评估误差的gap相比，还是有收益的。

### 2. Challenges in Neural Network Optimization

#### 2.1 Ill-Conditioning
“病态”指的是Hessian矩阵的处于病态。其发端是是梯度下降法。
对于一个优化目标f(x)，我们对其进行二阶泰勒展开，有：
$$
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^Tg + \frac{1}{2}(x - x^{(0)})^TH(x - x^{(0)}) \tag{4}\label{4}
$$

如果我们的学习率是$\epsilon$，那么一次优化迭代的步长是$-{\epsilon}g$，一次优化迭代后的点为$x^{(0)}-{\epsilon}g$，带入上式，得：
$$
f(x^{(0)}-{\epsilon}g) \approx f(x^{(0)}) - {\epsilon}g^Tg + \frac{1}{2} {\epsilon}^2 g^THg \tag{5}\label{5}
$$

式$\eqref{5}$中等号右边的三项分别是：函数的原始值、函数斜率导致的预期改善、函数曲率导致的校正。
在一次迭代中，$g^THg$：
* 如果为零，优化目标f(x)的下降程度跟我们的预期是一样的，
* 如果为负，f(x)的下降比我们预期得更多，
* 如果为正，f(x)的下降比我们预期得要少。

而如果式$\eqref{5}$中的后两项加起来大于零，即：
$$
\frac{1}{2} {\epsilon}^2 g^THg > {\epsilon}g^Tg
$$
那么一次迭代，目标函数不会下降反而会上升，这就是病态了。

`在很多情况中，梯度范数不会在训练过程中显著缩小，但是g^THg的增长会超过一个数量级。其结果是尽管梯度很强，学习会变得非常缓慢，因为学习率必须收缩以弥补更强的曲率。`

#### 2.2 Local Minima

1. 任何凸优化的问题都可以简化为寻找局部极小点，所有的局部极小点都是全局最小点。
2. 有些凸优化的底部是一个平坦的区域，其中任何一个点都是可以接受的。
3. 神经网络，是非凸的，会存在多个局部极小值，但是通常这些局部极小值并不是主要的问题。
	- 这多个局部极小值，可能是由模型的不可辨识性带来的，在这些局部极小值点，都有相同的代价函数值。
	- 如果局部极小值比全局最小值点大很多，那么会是个问题，但是现在学者们现在猜想，对于足够大的神经网络而言，大部分局部极小值都具有很小的代价函数，而通常我们只是要寻找一个很小的代价函数，而不一定要是最小的。

一种判断是否遇到了局部极小值的办法，是画出梯度的范数随迭代的变化，如果梯度范数没有缩小到一个微小的值，那么该问题既不是局部极小值，也不是其他临界点。
不过，在高维空间，即使范数很微小，也有可能不是局部极小值带来的。

#### 2.3 Plateaus, Saddle Points and other Flat Regions

对于高维非凸函数，鞍点比局部极小值点更常见。
`鞍点激增对于训练算法来说有哪些影响呢?对于只使用梯度信息的一阶优化算 法而言，目前情况还不清楚。鞍点附近的梯度通常会非常小。另一方面，实验中梯度 下降似乎可以在许多情况下逃离鞍点。`

#### 2.4 Cliffs and Exploding Gradients
多层神经网络通常存在像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。直接梯度更新，情况会非常糟糕，会使参数弹射很远，之前的优化都白做了。
所以现在一般的优化算法都会在迭代上加bound，也就是梯度阶段，来避免这个问题。

#### 2.5 Long-Term Dependencies

#### 2.6 Inexact Gradients

#### 2.7 Poor Correspondence between Local and Global Structure
基本上所有训练神经网络的方法，都是基于局部较小更新，而局部较小更新，可能存在的问题是：
1. 由于有些梯度只能近似计算，局部下降不能确定通往有效解的合理路径。
2. 由于病态，悬崖等问题的存在，即使路径正确，局部下降的步数也可能会非常多。
3. 在一些平坦区域上，局部下降不能找到有效的下降方向。
4. 在有些情况下，可能移动得太过贪心，导致反而远离了有效的解。

因此，寻找合适的初始化点，很重要。

#### 2.8 Theoretical Limits of Optimization

### 3. Basic Algorithms
### 4. Parameter Initialization Strategies
### 5. Algorithms with Adaptive Learning Rates
### 6. Approximate Second-Order Methods
### 7. Optimization Strategies and Meta-Algorithms
