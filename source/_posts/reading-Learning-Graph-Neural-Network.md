---
title: reading < Learning Graph Neural Network >
date: 2020-06-27 16:13:29
mathjax: true
tags:
  - GNN
categories:
  - study notes

---
《深入浅出图神经网络》的读书笔记。感觉这本书比较一般，很多地方讲得不够深入和清楚。

<!--more--->

# 第5章 图信号处理与图卷积神经网络

## 5.1 矩阵乘法的三种方式
掌握理解矩阵乘法的三个视角，对于理解矩阵运算很有帮助。对于C（K\*P） = A(K\*M)B(M\*P):
1. 內积视角：我最熟悉的视角，C中的某个元素：
$C_{ij} = A_{i,:}B_{:,j}$

2. 行向量视角：将B视作一堆行向量组成的矩阵，A是系数矩阵，C中的某一个行向量：
$C_{i,:} = \Sigma_{m} A_{im}B_{m,:}$
A中的行号是固定的i，B中的向量长度是统一的P

3. 列向量视角：将A视作一堆列向量组成的矩阵，B是系数矩阵，C中的某一个列向量：
$C_{:,j} = \Sigma_{m} B_{mj}A_{:,m}$
B中的列号是固定的j，A中的列向量长度是统一的K

## 5.2 图信号与图的拉普拉斯矩阵
图信号：定义在节点上的信号，图上的每个节点都有一个实数表示信号强度。由图上各节点的信号强度和节点之间的拓扑结构组成。
研究图信号的一个核心工具是拉普拉斯矩阵：
$L = D - A$
其实质是描述某节点与其邻接节点之间的信号差异的。
$L(\bar{x}) = (D-A)\bar{x} = [..., \Sigma(x_{i}-x_{j}),...]$
其二次型就是图的总变差：
$TV(x) = x^{T}Lx = \Sigma_{e_{ij}\in E}(x_{i} - x_{j})^2$

## 5.3 图傅里叶变换
### 5.3.1 关于拉普拉斯矩阵的几个性质：
1. L是一个实对称矩阵，可以被正交对角化
$L = V \Lambda V^T$
$V = [v_1, v_2, ... , v_N]$
是L的N个特征向量，是一个正交矩阵，$\lambda$是特征值。
2. L是一个半正定矩阵
由于其二次型$\mathbf{x}^TL\mathbf{x} = \Sigma_E(x_i - x_j)^2 \ge 0$，所以其是一个半正定矩阵。
其所有的特征值均大于等于0.
并且$LI = 0$（所有节点信号都是1，这里I是单位向量）,因此L有最小的特征值0，所以$\lambda_1 = 0$。
另外可以证明，对于$L_{sym}$，其特征值存在一个上限：$\lambda_N\le2$

### 5.3.2 图傅里叶变换
GFT:
对于一个图上的信号$\vec{x}$:
$\tilde{\mathbf{x}} = V^T\mathbf{x}$
$\tilde{x}_k$ 是$\mathbf{x}$在第k个傅里叶基上的傅里叶系数。
IGFT:
$V\tilde{\mathbf{x}} = VV^{T}\mathbf{x} = I\mathbf{x} = \mathbf{x}$
即：
$\mathbf{x} = V\tilde{\mathbf{x}}$

### 5.3.3 图傅里叶变化与图信号频率的关系
总变差描述了图的整体平滑程度，把GFT带入进去之后，有：
$TV(x) = \Sigma^N_k\lambda_k\tilde{x}_k^2$

即，图的总变差是图素有特征值的一个线性组合，权重是图信号对应的傅里叶系数的平方。
如果要找一组变差最小的图信号，那就分别是各傅里叶系数为1时候对应的图信号。
特征值依次排列在一起，对图信号的平滑度做出了一种梯度刻画，因此可以讲特征值等价成频率，描述对应的傅里叶基变化的剧烈程度。特征值越低，频率越低，对应的傅里叶基在总变差中变化得越缓慢，相近节点上的信号值趋于一致。vise vice。
总结一下就是：
**一个图信号的傅里叶变化，就是把一个图信号分解到L的各特征向量上，特征值描述了某一分量的平滑程度，等价于频率，傅里叶系数，就是对应频率的幅度。**

傅里叶系数合在一起，就是频谱了。频谱定了，图信号就定了。
频域是一种全局视角，即包括图的结构信息，有考虑了图信号本身值的大小。

## 5.4 图滤波器
图滤波器，就是在图信号的各个频率分量上对其强度进行一定程度的控制，如低通，高通，带通。
就是把$\lambda_i$ 变为 $h(\lambda_i)$

整个过程是：
1. 通过GFT，把图信号x变换到频域：$\tilde{\mathbf{x}} = V^T\mathbf{x}$
2. 通过$\Lambda_h = \Sigma^K_{k=0}h_k\Lambda^k$对各频率分量的强度进行调节，得到$\mathbf{\tilde{y}}$
3. 通过IGFT进行逆变换，$\mathbf{y} = V\mathbf{\tilde{y}}$


$\mathbf{y} = H\mathbf{x} = V(\Sigma^K_{k=0}h_k\Lambda^k)V^T\mathbf{x}$

这节课讲得并不是特别清楚。

## 5.5 图卷积神经网络
图卷积操作等价于图滤波操作。
图滤波操作的核心在于频率响应矩阵。
对频率响应矩阵进行参数化，就可以定义如下神经网络层：
$X' = \sigma(V diag(\theta)V^TX) = \sigma(\Theta X)$

$\sigma()$是激活函数，$\theta$是要学习的参数。
问题，计算量大，$\theta$的维数等于节点的个数，很容易过拟合。
改进办法：展开成多项式形式，只取前K维。

$X' = \sigma(V diag(\Phi\theta)V^TX)$
问题：依赖特征分解，计算负责度高。
改进办法：(其实中间还有一个chebnet)
1. 令K=1:
$X' = \sigma(\theta_0X+\theta_1LX)$
2. 令$\theta_0 = \theta_1 = \theta$:
$X' = \sigma(\theta(I+L)X) = \sigma(\theta\tilde{L}X)$
3. 对$\tilde{L}$进行归一化，引入权重矩阵
$X' = \sigma(\tilde{L}_{sym}XW) $

这就成了GCN层了。

# 第6章 GCN的性质
## 6.1 GCN与GNN的联系
两者都是局部连接。
两者卷积核的权重是处处共享的。
感受域随着卷积层的增加而变大。
## 6.2 GCN能对图数据进行端对端学习
## 6.3 GCN是一个低通滤波器
## 6.4 GCN的问题--过平滑


# 第7章 GNN的变体与框架

## 7.1 GraphSAGE
空域角度下的GNN。
主要两个思想：
1. 对邻居进行采样，而不是全部计算
2. 对采样后的邻居进行聚合

## 7.2 GAP
